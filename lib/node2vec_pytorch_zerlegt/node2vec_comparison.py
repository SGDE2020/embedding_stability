# this file is used to compare the embeddings of node2vec generated by node2vec_generator.py with respect to the effects of each piece of randomness

import numpy as np
import time
from tqdm import tqdm,trange
import argparse
import sklearn.neighbors # ballTree for computing nearest neighbors
import itertools # pairs of elements of a list
import scipy.linalg # procrustes
import sklearn.metrics.pairwise # cosine_similarity (pairwise)
import scipy.spatial.distance # cosine distance function

def define_parser():
  parser = argparse.ArgumentParser()
  parser.add_argument('-a', '--analysis', choices=['walks', 'neg', 'shuffle', 'train', 'all'],  default='all', help='Which task to evaluate')
  parser.add_argument('--no_overlap', action="store_true", default=False, help='select this to skip overlap computation')
  parser.add_argument('--no_aligned_cosine', action="store_true", default=False, help='select this to skip aligned cosine computation')
  parser.add_argument('--default_seed', type=int, default=2020, help='seed that is used for the other randomness dimensions')
  parser.add_argument('-n', '--number_of_embeddings', type=int, default=30, help='number of embeddings to evaluate')
  parser.add_argument('--path', type=str, default='embeddings', help='path where the embeddings are stored')
  parser.add_argument('--dataset', choices=['Cora', 'BlogCatalog', 'embedding'], default='embedding', help='Dataset (embedding filename)')
  parser.add_argument('--baseline', action='store_true', help='evaluate baseline')
  parser.add_argument('--walk_length_experiment', action='store_true', 
    help='evaluate the experiment on the walk length, ignores everything but the no_overlap and no_aligned_cosine flags (and default_seed)')
  args = parser.parse_args()
  return args

# hardcoded parameters for the experiment on the influence of the walk length
walk_length_experiment = {
  'embeddings' : 10,
  'default_seed' : 2020,
  'min_length' : 10,
  'max_length' : 150,
  'increment' : 5,
  'dataset' : 'Cora',
  'path' : 'embeddings_walk_length',
}

def load_embeddings(analysis,args):
  # index: the position in the filename where the different seeds occur
  embeddings = []
  description = "loading embeddings from disk"

  if analysis=='walks':
    for i in trange(args.number_of_embeddings, desc=description):
      filename = f'{args.dataset}_w{i+1}_n{args.default_seed}_s{args.default_seed}_t{args.default_seed}.csv.gz'
      current_embedding = np.loadtxt(args.path+"/"+filename)
      embeddings.append(current_embedding)

  if analysis=='neg':
    for i in trange(args.number_of_embeddings, desc=description):
      filename = f'{args.dataset}_w{args.default_seed}_n{i+1}_s{args.default_seed}_t{args.default_seed}.csv.gz'
      current_embedding = np.loadtxt(args.path+"/"+filename)
      embeddings.append(current_embedding)

  if analysis=='shuffle':
    for i in trange(args.number_of_embeddings, desc=description):
      filename = f'{args.dataset}_w{args.default_seed}_n{args.default_seed}_s{i+1}_t{args.default_seed}.csv.gz'
      current_embedding = np.loadtxt(args.path+"/"+filename)
      embeddings.append(current_embedding)

  if analysis=='train':
    for i in trange(args.number_of_embeddings, desc=description):
      filename = f'{args.dataset}_w{args.default_seed}_n{args.default_seed}_s{args.default_seed}_t{i+1}.csv.gz'
      current_embedding = np.loadtxt(args.path+"/"+filename)
      embeddings.append(current_embedding)

  if args.baseline or analysis=='baseline':
    for i in trange(args.number_of_embeddings, desc=description):
      filename = f'{args.dataset}_w{i+1}_n{100+i+1}_s{200+i+1}_t{300+i+1}.csv.gz'
      current_embedding = np.loadtxt(args.path+"/"+filename)
      embeddings.append(current_embedding)

  if args.walk_length_experiment:
    for length in trange(walk_length_experiment['min_length'], walk_length_experiment['max_length']+1, walk_length_experiment['increment'], desc=description):
      embeddings_single_length = []
      for i in range(walk_length_experiment['embeddings']):
        filename = f'{walk_length_experiment["dataset"]}_w{i+1}_n{args.default_seed}_s{args.default_seed}_t{args.default_seed}_len{length}.csv.gz'
        current_embedding = np.loadtxt(walk_length_experiment['path'] + "/" + filename)
        embeddings_single_length.append(current_embedding)
      embeddings.append(embeddings_single_length)

  return embeddings

# this function resets all relevant random counters
def reset_random(seed=0):
    random.seed(seed)
    np.random.seed(seed)


def compute_overlap_score(embeddings,k=20,jaccard=False,):
  # this function computes the overlap between the two given embeddings
  # returns avg_{pairs of embeddings}(avg_{nodes}(overlap)), return value between 0 (no overlap) and k (complete overlap)

  neighbor_indices = []
  for embedding in tqdm(embeddings,desc='computing and querying ball trees'):
    tree = sklearn.neighbors.BallTree(embedding)

    # compute nearest neighbors for every node
    _,ind = tree.query(embedding,k=k+1) 
    neighbor_indices.append(ind)

  # for each pair of items in the list neighbor_indices:
  average_intersections = []
  for ind1,ind2 in tqdm(itertools.combinations(neighbor_indices,2),desc='compute overlap scores'):
    avg_intersection = pairwise_overlap(ind1,ind2) 
    average_intersections.append(avg_intersection) 

  return np.average(average_intersections)


# computes the overlap between the lists of neighbors of two graphs
# returns (float64) the average overlap over all nodes, ranging between 0 and k (identical neighborhoods everywhere)
def pairwise_overlap(neighbors_list1,neighbors_list2,k=20):
  total_intersections = 0
  # for each node in the graph (for which the neighbors are given in the list)
  for i in range(len(neighbors_list1)):
      # compute the overlap
      overlap = np.intersect1d(neighbors_list1[i],neighbors_list2[i],assume_unique=True)
      total_intersections += len(overlap)-1 # every node intersects itself
  average_intersection = total_intersections / len(neighbors_list1)
  return average_intersection


def aligned_cosine_similarity(embeddings):
    """ Performs orthogonal transformation (Procrustes problem) between two embeddings computes cosine and l2 distances between them
      returns average over embedding pairs of averages over all nodes of similarities.
      returns similarities, i.e. higher is better
    """
    timer_procrustes = 0
    timer_distances  = 0

    results_cosine = []
    results_l2     = []
    for emb1,emb2 in tqdm(itertools.combinations(embeddings,2),desc='compute aligned_cosine_similarity scores'):

        # compute transformation matrix Q such that W1Q ~= W2
        start_time = time.time()
        transformation_matrix,_ = scipy.linalg.orthogonal_procrustes(emb1,emb2)

        # transform the first embedding to look like the second
        aligned_embedding = emb1 @ transformation_matrix # the @ is matrix multiplication
        timer_procrustes += time.time()-start_time

        # compute cosine similarity over all nodes
        start_time = time.time()
        pairwise_cosine_distances = np.array([scipy.spatial.distance.cosine(emb1[i], emb2[i]) for i in range(len(emb1))])
          # note that sklearn.metrics.pairwise.cosine_similarity would perform a comparison between all pairs instead of once per node
        average_cosine_distance = 1 - np.average(pairwise_cosine_distances)
        results_cosine.append(average_cosine_distance)

        # compute l2-distances over all nodes ("diagonal" entries only)
        pairwise_l2_distances = np.array([np.linalg.norm(emb1[i] - emb2[i]) for i in range(len(emb1))])
        results_l2.append(1 - np.average(pairwise_l2_distances)) # this adds a single score, 1 minus distance is similarity.
        timer_distances += time.time() - start_time

    print(f"computing procrustes took {timer_procrustes:.1f}sec, distances {timer_distances:.1f}sec")
    return np.average(results_cosine),np.average(results_l2)


def jaccard_from_overlap(overlap, k=20):
  # jaccard similarity is defined as intersection/union, which is equivalent to the following:
  # overlap is the absolute overlap, i.e. a number between 0 and k.
  return overlap/(2 * k - overlap)


# perform the analysis and print the in-class variation
def perform_analysis(task,args,embeddings=None):
  # load embeddings if not given from the outside
  if embeddings == None:
    embeddings = load_embeddings(task,args)

  # compute overlap scores (pure and jaccard)
  overlap_score = -1
  if not args.no_overlap:
    overlap_score = compute_overlap_score(embeddings)
    print(f"the average overlap over the given embeddings is {overlap_score:.2f}, jaccard score {jaccard_from_overlap(overlap_score):.2f}.")

  # compute aligned cosine similarity
  cosine_score = -1
  l2_score = -1
  if not args.no_aligned_cosine:
    cosine_score, l2_score = aligned_cosine_similarity(embeddings)
    print(f'average cosine distance between a node in different embeddings is {cosine_score:.2f} and {l2_score:.2f} with l2 norm')
  return overlap_score,cosine_score,l2_score



def perform_walk_length_experiment(args):
  all_embeddings = load_embeddings('',args) # do not load anything for a 'normal' task
  results = []

  # for length in trange(walk_length_experiment['min_length'], walk_length_experiment['max_length']+1, walk_length_experiment['increment'], desc=description):
  for index,embeddings in enumerate(all_embeddings):
    # perform the experiment on one slice
    length = walk_length_experiment['min_length'] + index * walk_length_experiment['increment'] 
    print(f'now working on the embeddings with length {length}')

    overlap_score,cosine_score,l2_score = perform_analysis(None, args, embeddings=embeddings)
    jaccard_overlap = jaccard_from_overlap(overlap_score)
    results.append([length,jaccard_overlap,cosine_score])

  # store the results on disk
  header_string = 'length, overlap_score (range 0-1, higher is better), cosine_score (range 0-1, higher is better)'
  np.savetxt('results_walk_length_experiment.csv',results,header=header_string)



args = define_parser()
if args.walk_length_experiment:
  # this experiment uses its own parameters stored locally in a dictionary.
  perform_walk_length_experiment(args)
elif args.analysis == 'all':
  # perform all tasks, one after the other
  walk_overlap,walk_cosine,walk_l2          = perform_analysis('walks',args)
  neg_overlap,neg_cosine,neg_l2             = perform_analysis('neg',args)
  shuffle_overlap,shuffle_cosine,shuffle_l2 = perform_analysis('shuffle',args)
  # train_overlap,train_cosine,train_l2       = perform_analysis('train',args)
  base_overlap,base_cosine,base_l2          = perform_analysis('baseline',args)

  # output results (in addition to results printed during the analyses)
  print(f'walks: overlap {walk_overlap/20:.2f}% (jaccard: {jaccard_from_overlap(walk_overlap):.2f}), aligned cosine {walk_cosine:.3f} aligned l2 {walk_l2:.2f}')
  print(f'neg sample: overlap {neg_overlap/20:.2f}% (jaccard: {jaccard_from_overlap(neg_overlap):.2f}), aligned cosine {neg_cosine:.3f} aligned l2 {neg_l2:.2f}')
  print(f'shuffle: overlap {shuffle_overlap/20:.2f}% (jaccard: {jaccard_from_overlap(shuffle_overlap):.2f}), aligned cosine {shuffle_cosine:.3f} aligned l2 {shuffle_l2:.2f}')
  # print(f'train: overlap {train_overlap/20:.2f}% (jaccard: {jaccard_from_overlap(train_overlap):.2f}), aligned cosine {train_cosine:.3f} aligned l2 {train_l2:.2f}')
  print(f'base: overlap {base_overlap/20:.2f}% (jaccard: {jaccard_from_overlap(base_overlap):.2f}), aligned cosine {base_cosine:.3f} aligned l2 {base_l2:.2f}')

  # and print the output into a file (additionally)
  results_file = open(f'results_{args.dataset}.csv',mode='w')
  results_file.write(f'walks: overlap {walk_overlap/20:.2f}% (jaccard: {jaccard_from_overlap(walk_overlap):.2f}), aligned cosine {walk_cosine:.3f}\n')
  results_file.write(f'neg sample: overlap {neg_overlap/20:.2f}% (jaccard: {jaccard_from_overlap(neg_overlap):.2f}), aligned cosine {neg_cosine:.3f}\n')
  results_file.write(f'shuffle: overlap {shuffle_overlap/20:.2f}% (jaccard: {jaccard_from_overlap(shuffle_overlap):.2f}), aligned cosine {shuffle_cosine:.3f}\n')
  # results_file.write(f'train: overlap {train_overlap/20:.2f}% (jaccard: {jaccard_from_overlap(train_overlap):.2f}), aligned cosine {train_cosine:.3f}\n')
  results_file.write(f'base: overlap {base_overlap/20:.2f}% (jaccard: {jaccard_from_overlap(base_overlap):.2f}), aligned cosine {base_cosine:.3f}\n')
  results_file.close()
else:
  perform_analysis(args.analysis,args)
